 # Python-Week-1
Introduction

Python is a programming language widely used for developing web applications, mobile applications, artificial intelligence and games. Python is known for being easiest to use compared to other programming languages. Python code can be writen on IDEs(Intergrated Development Environment) like Pycharm, Visual Studio Code and Sublime. It can be installed on a machine using the Python.org website.


# Variables and Types

Variables are basic unit of program, they are assigned a value which can store a piece of information temporarily. A variable beginning with a number can not be used, but mostly variables can begin with a lower case letter and have upper case letters or underscores. Unlike other programming languages, in Python a user does not have to specify a variable type, a variable can be declared an then assigned a value, the value assigned can be the one determining the variable type, to learn of the variable type in Python one can use the type(variableName) function and then run the code, after the code has been executed the terminal will return then variable type where there it is an int, char,float or string based on the value you assigned when declaring it.

# Data Structures

Data structures allow the storage of list values in a single variable. Types of data structures are Lists, Tuples, Sets and Dictionaries.

# Operators

Operators are instructions that perform operations on variables and values in Python. They are used to manapulate actions on data, there are Mathematical, Logical and Membership operators. Arithmetic operator is the operator that is used for mathematical calculations, these operators include addition(+), division(/), subtraction(-), and multiplication(*).

Comparison operator evaluate two variables or values and produce a boolean result either true or false.

Logical operators include "and", "or" and "not" can be used on boolean values. The "and" operator returns true only if both operands are true, "or" operator return true if at least one operand is true, "not" operator negates the boolean value it operates on.

Membership operators arenused to check if a number or string exists in a given list or string, "in" and "not in" are examples of logical operators.

# Lists

Lists are data structures that can store multiple values whether strings, intergers or floats. To access values in a list, indexes are used. There is a method called slicing that helps filter values according to index numbers. If one does not want to display other values in the list, they just use slicing to show only the values from a certain index. Lists can be modified using various, this modification includes removing, inserting and appending values in the list. Lists can be declare using a variable that used square brackets [] and have values in inside of it.

# Tuples and Sets

Tuples and sets on not so much different from lists, the only difference is that when defining a tuple, values are defined inside of round brackets, tuples are immutable which means they cannot be modified in any form. When defining a set values are defined inside of curly braces.


# Dictionaries
Like, lists,tuples and sets, dictionaries are data structures for storing a series of values. To define a dictionary we use curly brackets, but the difference between defining a set and a dictionary is that a dictionary has a key and a value. A key a value are separated by  the a colon(:), a key will be on the left side and a value will be on the right.

# List Comprehensions

A list comprehension is an easy way to iterate through a list using a for loop, while also returning a copy of th list you iterating over. It also enables you to filter or apply functions to every item in a list.

# Dictionaries and Comprehension

Comprehension in dictionaries is used the same way it is used in lists, the minor difference is that in dictionaries you specify a key and a value.


# Control Flow

Control flow is a way of making decisions in programming. This can be done by testing using an if stament, if the if the condition is true then the code inside the if statement can be executed but a different decision is made if the condition is false in the if statement, this can be achieved by using an else statement below the if stament. 
There are different types of loops, namely while, do while,for and foreach loop, a for and a while are the most commonly used loops. 


# WEEK 2

# FUNCTIONS

* A function is a basic unit of a program.
* Functions are made of a name and parameters, which are denoted by the def statement.
* A function can be called along with it's parameters to execute it's results.

# *Args

args only works on positional arguments, should there be a key wordm we will get an error that say "unexpected key word argument".

# **Kwargs (Key Word arguments)

These type of arguments when included in a function they print results as a dictionary.

# CLASSES & OBJECTS

A class is an extensible program template for creating objects, providing initial values for state and implementations of behavior.
An object is an instance of class.

# ERROR HANDLING FUNDAMENTALS

Errors are thing we always experience in programming. So an error handling method it used in order to handle errors.
To handle errors we use the try, exception statement to catch the errors that may arrise while coding. There are different error types and python can handle them all either with the "except" key word and return the error encountered. In errror handling, the finally part will always execute regardless of the exception caught on the code.

There 3 types of errors, namely 
* TypeError
* ZeroDivisionError
* Exception error.

# WEEK 3

# USER STORIES
These are usualy scenarios from a user's perspective, emphasizing the user's goal and motivation rather than the application itself.They may be driven by a goal to achieve something specific. User stories focus on who, what and why of achieving a goal.

# USE CASES
A use case is a project planning tool depicting how the user can interact with the system to perform certain tasks and how the system can respond to the user.Use cases focus on the who, what and how of achieving a goal.

# PROJECT REQUIREMENTS

Functional requirements describe what the application shoud or should not do and are written as sentences starting with "the application must" or "the application shall".

Non-functional requirements describe how the application should accomplish its task. They focus qualities like maintainability, reliability and usability.

# ARCHITECTURE

This focuses on how the project will be structured based on the project requirements gathered along with user stories. Classes are created and the databases to store user information.




# DATA ---ANALYTICS

# WEEK 1
# Day 1

# Forces Creating Golden Age of Analytics

* Data
* Storage
* Computing

The role of data analytics is to transform data into actionable insights guiding decision making processes within the organization.

This is involves the following key responsibilities:

* Data Collection and Preparation
* Data Analysis
* Data Visualization and Storytelling
* Decision Support
* Collaboration and Communication
* Continuous Learning and Adaptation

# The Analytics Process

This process involves: 

* Data Acquisition
* Cleaning and Manipulation
* Analysis
* Visualization
* Reporting and Communication
* 
The analytics process is iterative meaning that it is a series of sequential actions, making it more accurate to think of them as a set of interrelated actions that may be revisted frequently while working with a dataset.

# Analytics Techniques

* Descriptive Analytics
* Predictive Analytics
* Prescriptive Analytics

# Machine Learning, Artificial Intelligence, and Deep Learning

Machine learning uses algorithms to discover knowledge in datasets can be applied to make informed decisions about the future.

Machine learning commonly adds value on these cases: 

* Segmentning customers and determining the marketing messages that will appeal to different customer groups.
* Discovering anomalies in system and application logs that may be indicative of a cybersecurity incident.
* Forecasting product sales based on market and environmental conditions.
* Recommending the next movie that a customer might wish to watch based on their past activity and the preferences of similar customers.
* Setting prices for hotel rooms far in advance based on forecasted demand.

# Artificial Intelligence

Includes any type of technique when one is attempting to get a computer system to imitate a human behavior. Though it may not be possible for a modern computer to function at the complex reasoning found in the human mind, one can try mimic some small portions of human behavior and judgement.

# Machine Learning

This is a subset of AI techniques. ML techniques attempt to apply statistics to data problems in an effort to discover new knowledge. In short ML techniques are AI techniques designed to learn.

# Deep Learning

This is a further subdivision of machine learning that uses quite complex techniques, known as neural networks, to discover knowledge in a particular way. It is a highly specialized subfield of ML most commonly used for imae, video and sound analysis.


# Data Governance

Data governance programs ensure that the organization has high-quality data and is able to effectively control that data.



# UNDERSTANDING DATA
# DATA TYPES

To understand data types, it is best first to understad data elements. A data element is an attribute about a person, or thing containing data within a range of values. Data elements also describe characteristics of activities, including orders, transactions and events.

# Tabular Data

It is a data organized into a table, made up of columns and rows. A table represents information about a single topic. The contents of each column contain values for the data element. Each row represents a record of a single instance of the table's topic.

# Structured Data Types

This data is tabular in nature and organized into rows and columns. A spreadsheet is an example of structured data type. Clearly defined column headings make spreadsheets easy to work with and understand. A cell is in s spreadsheed is where a column and a row intersect.

# Character 

The character data type limits data entry to only valid characters.Characters can include alphabets on the keyboard, as well as numbers. Depending on ones needs, multiple data types are available that can enforce character limits.

# Alphanumeric

It is most widely used data type for storing character-nased data. As the name implies, alphanumeric is appropriate when a data element consists of both numbers and letters. This data type is ideal for storing product stock-keeping units (SKUs).It is common in the retail clothing space to have a unique SKU for each item available for sale.

# Strong And Weak Typing

Strong typing is when technology rigidly enforces data types.A database column defined as numeric only accepts numerical values. You will get an error if you attemot to enter characters into a numeric column.

Weak typing loosely enforces data types. Spreadsheets use week typing to help make it easier for people to accompilish their work. Spreadsheets default to an automatic data type and accommodate practically any value. When a person specifies a data type, it is loosely enforced compared to a database. For example, with a numeric spreadsheet cell, the softwae does not stop you from entering and storing characters.


# Unstructured Data Types

Unstructured data is any type of data that does not fit neatly into the tabular model.
Examples of unstructured data include digital images, audio recordings, video recordings and open-ended survey responses. Analyzing unstructured data creates a wealth of information and insight.

# CATEGORIES OF DATA

# Quantitative vs Qualitative Data

Quantitative data consists of numeric values. Data elements whose values come from counting or measuring are quantitative. Quantitative data answers questions like "How many?" and "How much?"

Qualitative consists of frequent text values. Data elements whose values describe characteristics, traits, and attitudes are all qualitative. Qualitative data answers questions like "Why?" and "What?".

# Discrete vs Continuous Data

Numeric data comes in two different froms: discrete and continuous. 

A helpful way to think about discrete data is that it represents masurements that can't be subdivided.
Whole numbers represent descrete data, continuous data typically need a decimal pont.

Qualitative data is discrete, but quantitative data can be either discrete or continuous.

# Categorical Data

Text data with a known, finite number of categories is categorical. When considering an individual data element, it is possible to determine whether or not it is categorical.

# Dimensional Data

Dimensional modeling is an approach to arranging data to facilitate analysis. Dimensional modeling organizes data into fact tables and dimension tables. Fact tables store measurement data that is of interest to a business. Dimensions are tables that contain data about the fact.

# COMMON DATA STRUCTURES

To facilitate analysis, data needs to be stored in a consistent, organized manner. When considering structured data, several concepts and standards inform how organize data. On the other hand, unstructured data has wider variety of storage approaches.

# Structured Data

Tabular data is structured data, with values stored in a consistent, defined manner, organized into columns and rows. Data is consistent when all entries in a column contain the same type of value. 

# Unstructured Data

It is a qualitative data, describing the characteristics of an event or object. Images, phrases, audio or video recordings, and descriptive text are all examples of unstructured data. Machine data is a common source of unstructured data. Machine data has various sources, including Internet of Things devices, smartphones, tablets, personal computers and servers. Machines create digital footprints of their activity as they operate.

Object storage facilitates the storage of unstructured data. The key-value concept underpins the design of object storage. They key is a unique identifier, and the value is the unstructured data itself.

# Semi-Structured Data

It is the data that has structure and that is not tabular. Email is an example of semi-structured data. Every email message has structural components, including recepient, sender, subject, date, and time. The body of an email is unstructured text, while attachments could be anything type of file.


# COMMON FILE FORMATS

Common file formats facilitate data exchange and tool interoperability. Several file formats have emerged as standards and are widely adopted.

# Text Files

They are most commonly used data file formats, they consist of plain text and are limited in scope to alphanumeric data. They ability to ve ioebed regardless of platform or operating system without needing a proprietary piece of software is they reason they are widely used. They are commonly referred to as flat files. When machines generate data, the output is commonly stored in a text file.

# Fixed-Width Files

# Javascript Object Notation (JSON)

It is an open standard file format, designed to add structure to a text file without incurring significant overhead. One of the design principles is that JSON is easily readable by people and easily parsed by modern programming languages. Languages such as Python, R and Go have libraries containing functions that facilitate reading and writing JSON files.

# Extensible Markup Language (XML)

It is a markup language that facilitates structuring data in a text file. It is conceptually similar to JSON, but it incurs more overhead because it makes extensive use of tags. Tags help readability, but add significant amount of overhead, tags describe a data element and enclose each value for each data element.

# Hypertext Markup Languag (HTML)

It is a markup language for documents designed to be displayed in a web browser. HTML pages serve as the foundation for how people interact with the World Wide Web. Similar to XML, HTML is a tag-based language.


# Key Points

- When dealing with data, you need to think through the data values you are working with because doing so influences your choice of data type.
- When using structured data you may be working with dates,numbers, texts, currency or alphanumeric data.
- When you have to incorporate metadata or represent a complex data structure, you need capabilities beyond what flat-file provides. Formating the data as JSON or XML is a viable alternative.


# CHAPTER 3
# EXPLORING DATABASE

When an organization needs to store data, there are various database options to choose from. While many database products exist, they belong in one of two categories:
- Relational
- Nonrelational

# Relational Database

Relational databases are pieces of software that let you make an perational system out of an Entity Relationship Diagram (ERD). Relational entities correspond to database tables, and entity attributes correspond to table columns.

# Nonrelational Databases

Relational database does not have a predefined structure based on tabular data. The result is a highly flexible approach to storing data. However, the data types available in relational databases are absent. As a result, you need to know more about the data itself to interact with it. Data validation happens in code, as opposed to being done in the database.

Nonrelational database examples include key-value, document, column family, and graph.

# Key-Value

Data is stored as a collection of keys and their corresponding values. A key must be globally unique across the entire database. The use of keys differs from a relational database, where a given key identifies an individual row in a specific table. The are no structural limits on the values of a key. A key can be a sequence of numbers, alphanumeric strings, or some other combination of values.The data that corresponds with the key can be any structured or unstructured data type.

# Document

A document is similar to key-value database, with additional restrictions. In a key-value database, the value can contain anything. With a document database, the value is restricted to a specific structured format. With a known, structured format, document databases have additional flexibility beyond what is possible with key-value.

# Column-Family

Column-family databases use an index to identify data in groups of related columns. The key in a relational database may be any type of ID but in a Column-family database the key becomes an index.


# Graph

Graph databases specialize in eploring relationships pieces of data. Relational models focus on mapping the relationships between entities. Graph models map relationships between actual pieces of data.


# DATABASE USECASES
# Online Transactional Processing (OLTP)

OLTP systems handle the transactions we encounter every day. This may include booking a flight reservation, ordering something online, or executing a stock trade. While the number of transactions a system handles on a given day can be very high, individual transactions process small amount of data. OLTP systems balance the ability to write and read data efficiently.

# Normalization

Normalization is a process for structuring a database in a way that minimizes duplication of data. First normal form(1NF) is when every row in a table is unique and every column contains a unique value. Second normal form(2NF) starts where 1NF leaves off. In addition to each row being unique, 2NF applies an additional rule stating that all nonprimary key values must depend on the entire primary key. 

Third normal form(3NF) builds upon 2NF by adding a rule stating all columns must depend on only the primary key.

# Online Analytical Processing

OLAP systems focus on the ability of organizations to analyze data. While OLAP and OLTP databases can both use relational database technology, their structures are fundamentally diffent. OLTP databases need to balance transactional read and write performance, resulting in a highly normalized design. Typically, OLTP databases are in 3NF.

On the other hand, databases that power OLAP systems have a denormalized design. Instead of having data distributed across multiple tables, denormalization results in wider tables than those found in a OLTP database. It is more efficient for analytical queries to read large amounts of data for a single table instead of incurring the cost if joining multiple tables together.


# Schema Concepts

The design of database schema depends on the purpose it serves. Transactional systems require highly normalized databases, whereas a denormalized design is more appropriate for analytical systems. A data warehouse is a database that aggregates data from many transactional systems for analytical purposes. Transactional data may come from systems that power the human resources, sales, makerting, and product divisions. A data warehouse facilitates analytics across the entire company.

A data mart is subset of a data warehouse. Data warehouses serve the entire organization, whereas data mart focus on the need of a particular department within the organization.

A data lake stores raw data in its native format instead of conforming to a relational database structure. Using a data lake is more complex than a data warehouse or data mart, as it requires additional knowledge about the raw data to make it analytically useful. Relational databases enforce structure that encapsulates business rules and business logic, both of which are missing in a data lake.


# Star Schema

`The star schema is designed to facilitate analytical processing, gets its name from what the schema looks like when looking at its entinty relationship diagram.

# Snowflake Schema

Snowflake and star schemas are conceptually similar in that they both have a central fact table surrounded by dimensions. Where the approachs differ differ is in the handling of dimensions. With a star, the dimension tables connect directly to the fact table. With a snowflake schema is less denormalized than the schema.

A snowflake schema query is more complex than the equivalent query in a star schema. Part of the trade-off is that a snowflake schema requires less storage space than a star schema. Data warehouses often use snowflake schemas, since many different systems supply data to the warehouse. Data marts are comparatively less complicated, because they represent a single data subject area. As such, data marts frequently use a star-schema approach.

# Dimensionality

Dimensionality refers to the number of attributes a table has. The greater the number of attributes, the higher the dimensionality. A dimension table provides additional context around data in fact tables.

# DATA ACQUISITION CONCEPTS

# Integration

The data can be transferred efficiently in various methods. One approach is known as extract, transform, and load(ETL). This method consist of three phases:

- Extract: In this phase, you extract data from the source system and place it in a staging area. The goal of the extract phase is to move data from a relational database into a flat file as quickly as possible.

- Transform: This phase transforms the data. The goal is to reformat the data from its transactional structure to the data warehouse's analytical design.
- Load: The purpose of the load phase is to ensure data gets into the analytical system as quickly possible.

Extract, Load and Transform is a variant of ETL. With ETL, data is extracted from a source database and loaded into a data warehouse. Once the extract and load phases are complete, the transformation phase gets underway. One key difference between ETL and ELT is the technical component performing the transformation. With ETL, the data transformation takes plce external to a relational database, using a programming languages like Python. ETL uses SQL and the power of a relational database to reformat the data.

ELT has an advantage in the speed with which data moves from the operational to the analytical database. Suppose you need to get massive amount of transactional data into an analylitical environment as quickly as possible. In that case, ELT is a good choice, especially at scale when the data warehouse has a lot of capacity. Whether you choose ETL or ELT is a function of organization need, staff capabilities, and technical strategy.

# ETL Vendors

Whether one chooses ETL or ELT for loading their data warehouse, they do not have to write transformations by hand. Many products support ETL or ELT. It is vital to evaluate the available free and paid options to determine the one that best fits the needs and system architecture goals.

# Data Collection Methods

Augmenting data from transactional systems with external data is an excellent way improve the analytical capabilities of an organization. To improve the accuracy of an analysis, one wants to include data about the weather, tourism, and their competitors. This data can come from various sources, including federal and state open data portals, other public data sources, and private purveyors of data.

- Application Programming Interface(API)
- Web Services
- Web Scraping
- Human-in-the-Loop
- Surveys
- Survey Tools
- Observation
- Sampling

# WORKING WITH DATA

# Data Manipulation
When manipulating data, one of four possible actions occur:

- Creae new data
- Read existing data
- Update existing data
- Delete existing data

SQL SELECT statement

SELECT <what>
FROM <source>

The SELECT clause identifies the from the table(s) that are retrieved. If you want to list the name and animal type from Table 3.1.

SELECT Animal_Name, Breed_Name

The FROM clause in a query identifies the source of data, which is frequently a database table. Both the SELECT and FROM clauses are required for a SQL statement to return data, as follows:

SELECT Animal_Name, Breed_Name
FROM Animal

# SQL Considerations

The keywords in SQL are case-insensitive. However, the case-sensitivity of column names and values depend on the database configuration.

Consider the following query:

Select Animal_Name, Breed_Name from Animal

The previous query returns the same results as this query:

SELECT Animal_Name, Breed_Name FROM Animal

SQL can also span multiple lines. For example, rewriting the previous query as follows will return identical results:

SELECT Animal_Name, Breed_Name

FROM  Animal

How a query appears is a function of organizational conventions. Factors that influence convention include database configuration, query efficiency, and how easy it is for people to read and understand the query.

# Filtering

Examining a large table in its entirety provides insight into the overall population. To answer questions that an organization's leadership has typically requires a subset of overall data. Filtering is a way to reduce the data down to only the rows that you need.

To filter data, you add a WHERE clause to a query. Note that the column you are filtering on does not have to appear in the SELECT clause.

# Sorting

When querying a database, you frequently specify the order in which you want your results to return. The ORDER BY clause is the component of a SQL query that makes sorting possible. Similar to how a WHERE clause performs, you do not have to specify the columns you are using to sort the data in the SELECT clause. The ASC keyword at the end of the ORDER BY clause sorts in ascending order whereas using DESC with ODER BY sorts in descending order. If you are sorting on multiple columns, you can use both ascending and descending as appropriate. Both the ASC and DES keywords work across various data types, including date, alphanumeric, and numeric.

# Logical Functions

They can make data substitutions when retrieving data. 

The IFF function expects the following three parameters:
- Boolean Expression: The expression must return either TRUE of FALSE.
- True Value: If the boolean expression returns TRUE, the IFF function will return this value.
- False Value: If the boolean expression returns FALSE, the IFF function will return this value.

The IFF function has the following syntax:
IFF(boolean_expression, true_value, false_value)
Example SELECT Animal_Name, IFF(Sex = 'M','Male', 'Female')

IFF is just one example of a logical function. When using functions, you need to balance their convenience with the knowledge that you are replacing data from the database with the function's coded balues. The ability to do this type of substitution is a real asset when dividing data into categories.

# Aggregate Functions

Summarized data helps answer questions that executives have, and aggregate functions are an easy way to summarize data. Aggregate functions summarize a query's data and return a single value.
Aggregate functions that are common across platforms:

- COUNT: Returns the total number of rows of a query.
- MIN: Returns the minimum value from the results of a query. NB - This works on both alphanumeric and numeric data types.
- MAX: Returns the maximum value from the results of a query. NB - This works on both alphanumeric and numeric data types.
- AVG: Returns the mathematic average of the results of a query.
- SUM: Returns the sum of the results of a query.
- STDDEV: Returns the sample standard deviation of the results of a query.


# CHAPTER 4: DATA QUALITY

Exploring some common reasons for cleaning and profiling datasets.

# DATA QUALITY CHALLENGES

# Duplicate Data

Duplicate data occurs when data representing the same transaction is accidentally duplicated within a system.

# Redundant Data

Redundant data occurs when the same data elements exist in multiple places within a system. Frequently, data redundancy is a function of integrating multiple systems. For example, multiple source systems that perform different business functions and use shared data elements create the conditions for data redundancy. When a record changes in one system, there is no guarantee that its new value changes in another system. Since there us no certainty of data synchronization, a data element can have conflicting values across systems. When integrating multiple data sources, dealing with redundant data is a persistent challenge.

# Missing Values

Missing values is another issue impacting data quality. Missing values occur when you expect an attribute to contain data but nothing is there. They are also known as null values. A null value is the absence of a value. A null value is not a space, blank or other character. Some situations may make sense when allowing null values. 

# Invalid Data

Invalid data are outside the valid range for a given attribute. An invalid value violates a business rule instead of having an incorrect data type. As much, you haveto understand the context of system to determine whether or not a value is invalid. 

# Nonparametric Data

Nonparametric data is a data collected from categorical variables. Sometimes the categories indicate differantiation, and sometimes they have a rank order associated with them.

# Data Outliers

A data outlier is a value that differs signficantly from other observations in a dataset. With outliers, you need to understand why they exist and whether they are valid in the context of your analysis.

# Specification Mismatch

A specification describes the target value for a component. A specification mismatch occurs when an individual component's characteristics are beyond the range of acceptable values. In manufacturing, a specification's tolerance is crucial to maintaining quality. When data is invalid, it has values that fall outiside a given range. On the other hand, a specification mismatch occurs when does not conform to its destination data type. For example, you might be loading data from a file into a database. If the destination column is numeric and you have text data, you'll end up with a specification mismatch. To solve this mismatch, you must validate that the inbound data consistently maps to its target data type.

# Data Type Validation

Data type validation ensures that values in a dataset have a consistent data type. Programming languages, including python, SQL and R, all have data type validations. Use these functions to validate the data type for each column in a data file before attempting a database load. It's in your best interest to detect and remediate data type issues as early as possible to ensure data is ready for analysis.


# DATA MANIPULATION TECHNIQUES

# Recording Data

Recording data is a technique you can use to map original values for a variable into new values to facilitate analysis. Recording groups data into multiple categories, creating a categorical variable. A categorical variable is either nominal or ordinal. Nominal variables are variable with two or more categories  where there is no natural order of categories. Ordinal variables are categories with an inherent rank.

# Derived Variables

A derived variable is new variable resulting from a calculation on an existing variable. A derived variable does not have to be categorical.

# Data Merge

A data merge uses a common variable to combine multiple databasets with diffent structures into a single dataset. Merging data improves data quality by adding new varibales to your existing data. Additional variables make for a richer dataset, which positevly impacts the quality of your analysis. ETL processes commonly append data while tranforming data for use in analytical environments.

# Data Blending

Data blending combines multiple sources of data into a single dataset at the reporting layer. Data blending differs from ETL in that it allows an analyst to combine datasets in an ad hoc manner without saving the blended dataset in a relational database.


# Concatenation

It is the merging of separate variables into a single variable. Concatenation is a highly effective technique when dealing with a source system that stores components of a single variable in multiple columns. The need for concatenation frequently occurs when dealing with date and time data. It is also useful when generating address information. 

# Data Append

A data append combines multiple data sources with the same structure, resulting in a new dataset containing all the rows from the original datasets. When appending data, you save the result as a new dataset for ongoing analysis.

# Imputation

Imputation is a technique for dealing with missing values by replacing them with substitutes. When merging multiple data sources, you may end up with a dataset with many nulls in a given column. If you are collecting sensor data, it is possible to have missing values due to collection or transmission issues.

Approaches an analyst can use for imputing values:

- Remove missing data - Remove rows with missing values without impacting the quality of the overall analysis.
- Replace with Zero - Can replace missing values with a zero.
- Replace with overall Average - Instead of using a zero, you can compute average weight of all rows that have data and then replace missing data with a zero is contextual.
- Replace with Most Frequent(Mode) - Alternately, you can take the most frequently occuring value, call the mode, and use that as the constant.
- Closes value average - With this approach you can use the values from the rows beffore and after missing values.

# Reduction

When dealing with big data, it is frequently unfeasible and inefficient to manipulate the entire dataset during analysis. Reduction is the process of shrinking an extensive dataset without negatively impacting its analytical value. There are a variety of reduction techniques from which you can choose. Selecting a method depends on the type of data you have and what you are trying to analyze. Dimensionality reduction and numerosity reduction are two techniques for data reduction.

# Dimensionality Reduction

One reduction technique is dimensionality reduction, which removes attributes from a dataset. Removing attributes reduces the dataset's overall size.

# Numerosity Reduction

Nemerosity reduction reduces the overall volume of data. One way to reduce the volume of quantitative data is by creating a histogram. You can create a histogram in Python, R, and many visualization-specific tools. A histogram is a diagram made up of rectangles, or bars, that show how frequently a specific value occurs. When creating a histogram, you can conFigure the width of a rectangle to represent a range of values. Another approach to reduce the data is through sampling. Sampling is a technique that selects a subset of individual records from the initial dataset.

Sampling techniques:

- Aggregation - it is the summarization of raw data for analysis. Aggregating data provides answers that help make decisions.
 - Transposing data - it is when you want to turn rows into columns and columns into rows to facilitate analysis. When transposing, each value from a column becomes a new column.
 - Normalization - In this context, normalizing data converts data from diffent scales to the same scale. If you want to compare columns whose measurements use different units, you want to normalize the data.
 - Min-Max Normalization -
 - Parsing/String Manipulation -  Raw data can contain columns with composite or distributed structural issues. A composite issue is when a raw data source has multiple, distinct values combined with a single character column.

# MANAGING DATA QUALITY

# Circustance to Check for Quality

- Data Acquisition
- Data Transformation and Conversion
- Data Manipulation
- Final Product Preparation

# Automated Validation

Many data sources feed analytics environments. While some of these data sources are other computer systems, others depend directly on people. Whenever people interact with systems, it's possible to introduce data-related errors. Whether source is machine or human-generated, one way to prevent data entry mistakes from adversely impact data quality is to automate data validation checks.

# Data Quality Dimensions

Six dimensions to take into account when assessing data quality are accuracy, completeness, consistency, timelines, uniqueness and validity.

# Data Quality Rules and Metrics

With an understanding of data quality dimensions, one need to consider how to measure each of them  in the quest to improve overall quality.
Data conformity, which encompasses elements of accuracy, consistency, uniqueness and validity. Conformity and nonconformity of data can be assessed when consolodating data from multiple sources in an analytics environment. Noncomformity is when source data does not match the destination data type size and format. Noncomformity causes an ETL challenge. To validate conformity issues is to confirm how many rows pass successfully to the target and how many fail.


# Methods to Validate Quality

- Reasonable expectations - Determine whether or not the data in analytics environment meets reasonable expectations.
- Data profiling - Data profiling uses statistical measures to check for data discrepancies, including values that are missing, occuring either frequently or infrequently or that should be eliminated.
- Data audits - These look at your data and help you understand whether or not you have the data you need to operate your business.
- Sampling - This is a statistical technique in which you use a subset of your data to inform conclusions about your overall data.
- Cross-Validation - This is a statistical technique evaluating how well predictive models perform. 


# CHAPTER 5

# Fundamentals Of Statistics

- A census is when you obtain data for every element of your population. Collecting a sample is a cost-effective and time-effective alternate to gathering census data.
- In statistics a variable can be defined as a serial number that uniquely identifies a wiper switch.
- Univariate analysis is when you explore the characteristics of a single variable, independent of the rest of the dataset.
- An observation is an individual record in a dataset corresponding to a tabular data row.

In statistics, one needs to be considerate of the sample size.
A statistic is a numeric representation of a property of a sample
A population is a total amount of things.
A sample refers to a small part of the population that we examine to get the information from.

`Types of statistics:

- Inferential
- Descriptive

# DESCRIPTIVE STATISTICS

It is a part of statistics that summarizes and describe data. Descriptive statistics can be used a tool to help understand the characteristic of a dataset. When working with a dataset for the first time, you may perform univariate analysis to answer questions about a variable's value. Descriptive measures to develop summary information about all of a variables observations.

# Measures of Frequency

Measure of frequency helps understand how often something happens. When encountering data for the first time, it is vital to determine the size of the data you will be working with to help guide the analysis.

- Count is a way of understanding the amount of data being worked with to count the number of observations.
- Percentage is a frequency of measure that identifies the proportion of a given value for a variable with respect to the total number of rows in a  dataset. To calculate a percentage, you need the total number of observations and the total number of observations for a specific value of variable.
- Frequency describes how often a specific value for a variable occurs in a dataset. Frequency is typically explored when conducting univariate analysis.

# Measures of Central Tendency

Various measures of central tendency can be explored to help establish an overall perspective on a given dataset. Measures of central tendency can be used to identify the central, or most typical value in a dataset.

- Mean
- Median
- Mode

# Measures of Dispersion

Measures of dispersion can be used to create context around data spread.

Common measures of dispersion

- Range - a range of a variable is the difference between its maximum and minimum values. Understanding the range helps put the data you are looking at into context and can help determine what to do with outlier values. It also helps identify invalid values in your data.
- Distribution - It is a function that illustrates probable values for a variable and the frequency with which they occur.
- Normal Distribution - This is symmmetrically dispersed around its mean, which gives it a distinctive bell-like shape. Normal distribution is also know as a "bell curve" due to its shape.
- Skewed Distribution - It has asymmetrical shape, with a single peak and a long tail on the side. It has either right(positive(greater than median)) or left(negative(less than median)) skew.
- Bimodal Distribution - This has two distinct modes, and a multimodal distribution has multiple distinct modes.
- Variance - This a measure of dispersion that takes values for each observation in a dataset and calculates how far they are from the mean value. This measure indicates how spread out the data is in squared units.
Variance can be use a useful measure when considering financial investments.
- Standard Deviation - Measures dispersion in terms of far values of a variable are from its mean. It is the avarage deviation between individual values and the mean.

# Measures of Position


# INFERENTIAL STATISTICS

Inferential statistics is a branch of statistics that uses sample data to draw conclusions about the overall population.

# Confidence Intervals

Whenever a sample is taken from a population, the statistics generated are unique to the sample. A population as whole can be described by collecting a range of scores to make the inferences about the population as whole. A confidence interval describes the possibility that a sample  statistic contains the true population parameter in a range of values around the mean.

- Confidence Interval Considerations - Specifying the confidence level in addition to the sample mean, population standard deviation and sample size are thing that can be be considered when calculating a confidence. The confidence level is a percentage that describes the range around the mean.
- When calculating confidence intervals, one needs to have a the standard deviation of the entire population.

# HYPOTHESIS TESTING

A hypothesis is an approach of to proving or disproving ideas. Hypothesis test consists of two statements, only one of which can be true.

Components of hypothesis test: 
 - Null Hypothesis
 - Alternative Hypothesis

To determine the statistical significane of whether to accept or reject the null hypothesis, a test statistic against a critical value needs to be compared. A test statistic is a single numeric value describing how closely a sample data matches the distribution of data under the null hypothesis. The way the null and alternative hypotheses are are defined determines whether a one-tailed or two-tailed test is being conducted.

- One-tailed test is when the alternative hypothesis is trying to prove a difference in a specific direction.

# Null Hypothesis

This hypothesis presumes that there is no effect on the test you are conducting. When hypothesis testing, the default assumption is that the null hypothesis is valid and that there is evidence to reject it.


# Alternative Hypothesis

Alternative hypothesis presumes that the test being conducted has effect.

# Hypothesis Testing with the Z-test

This is appropriate when the sample at hand is over 30 and a known population standard deviation, and normal reduction is used.

# Hypothesis Testing with the T-test

T-test is similar to z-test but it uses the t-distribution instead of the standard normal distribution.

# Hypothesis Testing with the Chi-Square

Z-tests and t-tests work well on numeric data. The chi-square test is available when one needs to assess the association of two categorical variables. In this cas, the null hypothesis asserts that there is no association between the variables, and the alternative hypothesis states that there is an association between them.

# Simple Linear Regression

This analysis technique that explores the relationship between an independent variable and a dependent variable. Linear can be used to identify whether the independed is a good predictor of a dependent variable. Linear regression analysis can be performed on Microsoft Excel, Python and R. Plotting the results of a regression, the independent variable is on the x-axis and the dependent variable is on the y-axis. 

A crucial aspect of linear regression is the correalation between how far the observations are from the regression line. Correlation is a measurement of how well the regression line fits the observations. When evaluating the correlation between variables, one thing to keep in is that it does not imply causation.

# ANALYSIS TECHNIQUES

# Determine Type of Analysis

Understanding business objectives and desired outcomes is crucial when embarking on a new analytics challenge, as it informs the type of analysis being conducted. To understand objectives one needs to ensure that they have clarity on business questions at hand. A goal of answering questions is to develop an insight that informs a business decision.

While reviewing requirements, develop a list of clarifying questions. The list can help define the scope of the analysis. The list clarification can identify gaps between achieavable given data source and time constraints.

# Types of Analysis

- Trend analysis - seeks to identify patterns by comparing data over time.
- Performance analysis - examines defined goals and measures performance against them and can inform the development of future projections.
- Link analysis - is a technique that uses network theory to explore the relationship between data points.

# Exploratory Data Analysis(EDA)

EDA uses descriptive statistics to summarize the main characteristic of a dataset, identify outliers, and give you context for the future analysis.

Approaches to conducting EDA typically encompass the following:

- Check Data Structure - Check data is in the correct format for analysis.
- Check Data Representation - Validate data types and ensure that variables contain the data you expect.
- Check if Data is Missing -
- Identify Outliers - 
- Summarize Statistics
- Check Assumptions


# CHAPTER 6: DATA ANALYTICS TOOLS

# Spreadsheets

- Productivity software packages that allow users to create documents that organize any type of data rows and columns.
- They lack any of the constraints of a relational database.
- Spreadsheeds' power comes from the fact that virtually anybody can use one.

# Programming Languages

# R

- Tidyverse is a package for R
- Tidyverse simplifies the use of the language and makes it accessible to anyone willing to invest a few hours learning some basic syntax.

# Python

- Pandas is the data analysis library, it provides set of tools for structuring and analyzing data.

# Structured Query Language (SQL)

SQL is divided into two major sublanguages:

- Data Definition Language - Mainly used by developers and administrators. It used to define the structure of the database itself. Does not work inside a database but sets grounf rules for database to function. DDL commands: CREATE, ALTER, DROP
- Data Manipulation Language - Is the subset of SQL commands used to work with data inside a database. Does not change the database structure, but they add, remove and change the data inside a database. DML commands: SELECT, UPDATE, INSERT, DELETE

# STATISTICS PACKAGES

- IBM SPSS
- Stata
- Minitab

# MACHINE LEARNING

# IBM SPSS Modeler

It  is popular tool for building graphical machine learning models. Instead of requiring that users write code, it provides intuitive interface where analysts can select tastks that they would like software to carry out and then connect them in a flowchart-style interface.

# RapidMiner

Is a graphical machine learning tool that works in a manner similar to IBM SPSS Modeler. It offers access to hundreds of different algorithms that may be placed visually designed machine-learning workflow.


# DATA ANALYTICS SUITES

# IBM Cognos
 IBM Cognos is an example of one these integrated analytics suites. It uses a web-based platform to offer analysts with an organization access to their data and is backed by IBM's Watson AI capability.

 **Cognos Connection** - Is a web-based portal that offers access to other elements of the Cognos suite.
 **Query Studio** - Provides access to data querying and basic reporting tools.
 **Report Studio** - Offers advanced report design tools for complex reporting needs. 
 **Analysis Studio** - Enables advanced modeling advanced and analytics on large datasets.
 **Event Studio** -  Provides real-time data monitoring and alerting, allowing business leaders to be immediately notified when certain events take place and/or provide automated responses to those events.
 **Metric Studio** -  Offers the ability to create scorecards for business leaders to quickly analyze key metrics from across the organization.
 **Cognos Viewer** - Allows stakeholders to easily interact with data and analyses prepared using Cognos.

 # Microsoft Power BI

Power BI is Microsoft's analytics suite built on the company's popular SQL Server database platform.

Major component of Power BI:

**Power BI Desktop** - A windows application for data analysts, allowing them to interact with data and publish reports.
**Mobile apps** -  for Power BI provide users of iOS, Android, and Windows devices with access to Power BI capabilities.
**Power BI Report Builder** - allows developers to create paginated reports that are designed for printing, email, and other distribution methods.
**Power BI Report Server** - Offers organizations the ability to host their own Power BI environment on internal servers for stakeholders to access.

# MicroStrategy

An analytics suite that is well-known than similar tools from IBM and Microsoft, but it does have a well-established user base. It offers many of the same tools as its counterparts, making it easy for users to build dashboarfs and reports and apply machine learning techniques to their business data.

# Domo

Domo is a Software-a-a-Service(SaaS) analytics platform that allows businesses to ingest their data and apply a variety of analytic and modeling capabilities. 

# Datorama

Salesforce Datorama is an analytics tool that focuss on a specific component of an organization's business: sales and marketing. It is not a general purpose analytis tool but is instead focused on applying machine learning, visualization and other analytics techniques to the sales and marketing process.

# AWS QuickSight

It is a dashboarding tool available as part of the Amazon Web Services cloud offering. The tool's power comes from the fact that it is available on a pay-as-you-go basis and its integration with the powerful data storage, data warehousing, machine learning, and artificial intelligence capabilities offered by the Amazon cloud.

# Tableau

It is the most popular data visualization tool available in the market today. The focus of this is tool is on the easy ingestion of data from a wide variety of sources and powerful visualization capabilities that allow analysts and business leader to quickly identify trends in their data and drill down into specific details

# Qlik

Qlik is another popular SaaS analytics platform, offerin access to cloud-based analytics capabilities.

Qlik offers the following major product:

**QlikView** - Is the company's original analytics platform that focuses on providing rapid insights.
**Qlik Sense** - Is a more advanced platform providing more sophisticated analytics capabilities.

# BusinessObjects

It is an enterprise reporting tool from SAP that is designed to provide a comprehensive reporting and analytics environment for organizations. One of the strengths of this suite is the ability to integrate **BusinessObjects** reports with other applications, allowing organizations to integrate analytics into other portions of their workflow.


# CHAPTER 7: Data Visualization with Reports and Dashboards

# Understanding Business Requirements

Reports and Dashboards both summarize data for the end users, but they distribute those summaries in different ways.

- A **Report** is a static electronic or physical document that reflects information at a given point in time.
- A **Dashboard** is an interactive visualization that encourages people to explore data dynamically.

Both reports and dashboards are ideal tools for visualizing data content.

- A **Pull approach** is when you publish a report to known location, like a web page, and let people know the frequency and timing of when report updates.
- A **Push approach** is when the report automatically sent to the appropriate people as it becomes avaible.
- A **Blende approach** is when you inform people that the report is available while maintaining central control of the report itself.

# Understanding Report Design Elements

There 5 design principles known as "five Cs" of creating visualizations, will help ensure that your reports and dashboards communicate clearly and efficiently.

- **Control** has to do with how you focus the attention of your audience.
- **Correctness** makes sure that your information is accurate and that there are no spelling mistakes.
- **Clarity** refers to selecting the right viasualization tool for communicating your message, making sure the visualization is easy to interprete and visually crisp and using font sizes that easy to read.
- **Consistency** refers to using the same design and documentation elements throughtout your report or dashboard to give your visualizarion a cohesive and complete feel.
- **Concentration** refers to using visuals to focus your audience's attention on the most relevant information without overwhelming them with details.

# DATA GOVERNANCE
























































































